{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install llama_index.vector_stores.qdrant\n",
    "# !pip install llama_index.core\n",
    "# !pip install llama_index\n",
    "# !pip install llama_index.embeddings.huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Qdrant and found collection 'chat_with_docs'\n",
      "Loaded 32 documents\n",
      "Creating vector store...\n",
      "Setting up storage context...\n",
      "Building index from documents (this may take a while)...\n",
      "Index creation complete!\n",
      "Successfully created index!\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import qdrant_client\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Define collection name\n",
    "collection_name = \"chat_with_docs\"\n",
    "\n",
    "# Connect to Qdrant\n",
    "try:\n",
    "    client = qdrant_client.QdrantClient(\n",
    "        host=\"host.docker.internal\",\n",
    "        port=6333\n",
    "    )\n",
    "    # Test connection by getting collection info\n",
    "    try:\n",
    "        client.get_collection(collection_name)\n",
    "        print(f\"Successfully connected to Qdrant and found collection '{collection_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Collection '{collection_name}' not found. Creating it now...\")\n",
    "        # Get the dimension from your embedding model\n",
    "        embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                          trust_remote_code=True)\n",
    "        embedding_dimension = 1024  # bge-large-en-v1.5 uses 1024 dimensions\n",
    "        \n",
    "        # Create the collection with proper format using VectorParams\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(\n",
    "                size=embedding_dimension,\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "        print(f\"Created collection '{collection_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Qdrant: {e}\")\n",
    "    print(\"\\nPossible solutions:\")\n",
    "    print(\"1. Make sure Qdrant is running locally with: docker run -p 6333:6333 qdrant/qdrant\")\n",
    "    print(\"2. Check if port 6333 is not blocked by firewall\")\n",
    "    print(\"3. Verify no other application is using port 6333\")\n",
    "    raise\n",
    "\n",
    "# Load documents\n",
    "input_dir_path = './docs'\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=input_dir_path,\n",
    "    required_exts=[\".pdf\"],\n",
    "    recursive=True\n",
    ")\n",
    "docs = loader.load_data()\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "\n",
    "# Set up embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                  trust_remote_code=True)\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Create index function\n",
    "def create_index(documents):\n",
    "    print(\"Creating vector store...\")\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                    collection_name=collection_name)\n",
    "    \n",
    "    print(\"Setting up storage context...\")\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    print(\"Building index from documents (this may take a while)...\")\n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                           storage_context=storage_context)\n",
    "    \n",
    "    print(\"Index creation complete!\")\n",
    "    return index\n",
    "\n",
    "# Create the index\n",
    "try:\n",
    "    index = create_index(docs)\n",
    "    print(\"Successfully created index!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating index: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "\n",
    "# !pip install llama_index.llms.ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43990c1518b40fe97c414b7a44fd628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2f1c3b2589466b9edecd34e226cef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/62.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbcf0649e744bf793dd109e4be10202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042667b93a5d4dc2bc88552b9e0f5377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8920ae5c62143f6b2c0c3da1bba1503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is DSPy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DSPy stands for Deep Speech Processing with Yaml, which is a library and framework for natural language processing (NLP) in Python. It provides an easy-to-use interface for implementing various NLP tasks, including text processing, question answering, and more.\n",
       "\n",
       "In the context of this preprint, DSPy is being used to develop and train AI models for natural language understanding and generation. The library offers a range of modules and interfaces that allow developers to create custom NLP models, train them on large datasets, and fine-tune their performance using various optimization techniques.\n",
       "\n",
       "Some specific features of DSPy include:\n",
       "\n",
       "* Natural Language Signatures: DSPy allows users to define natural language signatures, which are concise and declarative specifications for how a model should be prompted or asked questions.\n",
       "* Module-based Programming: DSPy provides an interface for creating custom modules that can be used as building blocks for more complex models. These modules can be easily composed together to create more sophisticated models.\n",
       "* Bootstrapping: DSPy offers several bootstrapping techniques, including simple fine-tuning and multi-hop training, which allow developers to optimize their model's performance on specific tasks or datasets.\n",
       "\n",
       "The preprint also discusses two key components of DSPy: Predict and ChainOfThought. The Predict module is a basic building block that allows developers to define natural language signatures and train models using them. The ChainOfThought module extends the Predict module by adding additional logic for generating queries and responses, which can be used to create more complex models.\n",
       "\n",
       "Overall, DSPy provides a powerful and flexible framework for developing NLP models in Python, allowing users to easily define and implement their own natural language processing tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
